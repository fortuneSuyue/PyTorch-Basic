{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Networks\n",
    "\n",
    "### Contents\n",
    "    1. Introduction\n",
    "    2. MNIST Dataset\n",
    "    3. Define your own Neural Network\n",
    "    4. Train the Model\n",
    "    5. Test the Model\n",
    "    6. Demonstrate the Model\n",
    "    7. Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A neural network is a network composed of artificial neurons and nodes, being modeled after the human brain architecture and activation processes. It is mainly used to solve problems of artificial intelligence problems. <br/>\n",
    "\n",
    "<br> The below diagram describes a brief process of neural network. Inputs are multiplied by weights, and summed, and it is passed to activation function, which converts the output to a value in a specific range, -1 to 1 or 0 to 1.\n",
    "\n",
    "<img src = \"./Images/DNN.png\">\n",
    "\n",
    "There are variations of the neural network, including fully connected networks, convolutional neural networks, recurrent neural networks, that works in different areas. <br/>\n",
    "<br> **Fully-connected networks**, or deep neural networks, is comprised of linear networks. <br/>\n",
    "<br> **Convolutional Neural Networks (CNN)** is typically made up of convolutional layers, pooling layers, and fully-connected layers. It effectively extracts useful features from visual data, thus it is widely used in the field of visual tasks. <br/>\n",
    "<br> **Recurrent Neural Networks (RNN)** save all the pre-processed information so as to reuse it in the future, thus effective in dealing with natural language and time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import helper\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting device on GPU if available, else CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset\n",
    "\n",
    "A neural network introduced for this tutorial is for **digit classification** using `MNIST(Modified National Institute of Standard and Technology database)`. <br/>\n",
    "<br> It is a large database of handwritten digits and most widely used dataset for training and testing neural networks and for beginners. It contains 60,000 training images and 10,000 testing images and each image has a size of 28 * 28 (= 784) and is a binary image (black and white or 1-color channel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor() must be declared before Normalize()\n",
    "                           # It converts the range from [0, 255] to [0, 1]\n",
    "    transforms.Normalize(mean = (0.5, ), std = (0.5, ))\n",
    "    # mean: (m1, m2, ..., mn) and std: (s1, s2, ..., sn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size is the number of training examples in one forward and backward pass.\n",
    "# As the computer is a binary machine, the magnitude of batch size is recommended to set powers of two (2^n). \n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# download and load the train data\n",
    "train_data = datasets.MNIST('./data/MNIST', \n",
    "                            download = True, \n",
    "                            train = True, \n",
    "                            transform = transform)\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = True)\n",
    "\n",
    "# download and load the test data\n",
    "test_data = datasets.MNIST('./data/MNIST', \n",
    "                           download = True, # already downloaded so don't have to declare again\n",
    "                           train = False, \n",
    "                           transform = transform)\n",
    "\n",
    "test_loader = DataLoader(test_data, \n",
    "                         batch_size = batch_size, \n",
    "                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.size(), test_data.data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the data, the below code shows the random data within training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 7\n",
      "Size of Image: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = torch.randint(0, len(train_data), (1, )).item()\n",
    "random_image = train_data[idx][0].squeeze().numpy()\n",
    "target_num = train_data[idx][1]\n",
    "print(\"Target: {}\".format(target_num))\n",
    "print(\"Size of Image: {}\".format(random_image.shape))\n",
    "plt.imshow(random_image, cmap = \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define your own Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./Images/MNIST_Network.png\" align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> [Image source](http://neuralnetworksanddeeplearning.com/chap1.html) </br>\n",
    "\n",
    "Let's build a neural network that classifies this dataset with PyTorch. Its input should be 784 (= 28 * 28) and output 10. The figure above is for simple reference, hence does not match to the network built below. More layers or deep learning techniques can be added to increase the accuracy. <br/>\n",
    "<br> `nn.Sequential` is a 'sequential' container that includes `nn.Linear`, `nn.Conv2D`, and `nn.ReLU`. It processes in the order of methods that the user put on. <br/>\n",
    "<br> `forward` function processes the functions declared in `__init__`. Read the below code for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "\n",
    "input_size = 28 * 28 #(= 784)\n",
    "hidden_size = [512, 256, 64] # multi-layer perceptron\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size[0]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size[0], hidden_size[1]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size[1], hidden_size[2]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size[2], num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, objective funciton and optimizer are required. Objective function calculates the difference between the answer the output from the model. Optimizer updates the weight to derive outputs closer to answers during back propagation.<br/>\n",
    "<br> `torch.nn.CrossEntropyLoss` is used especially when solving classification problems.\n",
    "<br> `torch.optim.Adam` optimizer is used as it shows the best performance so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# train list\n",
    "train_cost = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model\n",
    "\n",
    "The designed neural network should be trained using MNIST dataset to achieve its original goal, derivation of desired output on un-seen data. The below diagram illustrates the brief overview of training of neural networks.\n",
    "\n",
    "<img src = './images/TrainingNN.png'>\n",
    "\n",
    "The input is fed into the neural networks. The, it outputs a prediction and it is compared with true targets using loss function. Then, the optimizer updates the weights of the network using the loss value.\n",
    "\n",
    "In addition, `num_epochs` is total number of epochs. \n",
    "<br> `model.train()` makes model to be train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = len(train_data) // batch_size\n",
    "    \n",
    "        for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "            X = batch_images.view(-1, 28 * 28)\n",
    "            Y = batch_labels\n",
    "            \n",
    "            #######################################################################\n",
    "            ### The below five or six lines should be memorized for further use.###\n",
    "            \n",
    "            # forward pass\n",
    "            pred = model(X)\n",
    "            # calculation of loss value\n",
    "            cost = criterion(pred, Y)\n",
    "            # Adding cost value to the cost_losses class for graph plot later.\n",
    "            train_cost.append(cost.item())\n",
    "            \n",
    "            ## backward pass and optimization\n",
    "            # gradient initialization\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            cost.backward()\n",
    "            # parameter update\n",
    "            optimizer.step()\n",
    "            \n",
    "            #######################################################################\n",
    "        \n",
    "            # print statistics of training process\n",
    "            if (i+1) % 300 == 0:\n",
    "                print('Epoch [%d/%d], lter [%d/%d], Loss: %.4f'\n",
    "                     %(epoch+1, num_epochs, i+1, total_batch, np.mean(train_cost)))\n",
    "    \n",
    "    # Save the model weights for future inference\n",
    "    torch.save(model.state_dict(), './data/Tutorial_2_BasicNN.pkl')\n",
    "    \n",
    "    print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], lter [300/937], Loss: 0.5591\n",
      "Epoch [1/7], lter [600/937], Loss: 0.4048\n",
      "Epoch [1/7], lter [900/937], Loss: 0.3391\n",
      "Epoch [2/7], lter [300/937], Loss: 0.2907\n",
      "Epoch [2/7], lter [600/937], Loss: 0.2622\n",
      "Epoch [2/7], lter [900/937], Loss: 0.2408\n",
      "Epoch [3/7], lter [300/937], Loss: 0.2210\n",
      "Epoch [3/7], lter [600/937], Loss: 0.2066\n",
      "Epoch [3/7], lter [900/937], Loss: 0.1958\n",
      "Epoch [4/7], lter [300/937], Loss: 0.1838\n",
      "Epoch [4/7], lter [600/937], Loss: 0.1759\n",
      "Epoch [4/7], lter [900/937], Loss: 0.1687\n",
      "Epoch [5/7], lter [300/937], Loss: 0.1610\n",
      "Epoch [5/7], lter [600/937], Loss: 0.1552\n",
      "Epoch [5/7], lter [900/937], Loss: 0.1499\n",
      "Epoch [6/7], lter [300/937], Loss: 0.1443\n",
      "Epoch [6/7], lter [600/937], Loss: 0.1399\n",
      "Epoch [6/7], lter [900/937], Loss: 0.1356\n",
      "Epoch [7/7], lter [300/937], Loss: 0.1310\n",
      "Epoch [7/7], lter [600/937], Loss: 0.1274\n",
      "Epoch [7/7], lter [900/937], Loss: 0.1243\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "train(model = model, num_epochs = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition of more deep learning techniques should improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we should evaluate the model using unseen data. First, use `load_state_dict(torch.load()` to load the saved model weights. Then, run `model.eval()` to set the model to evaluation mode. It turns off any drop-out or batch normalization layers in the model that are not desriable to be used during the test. Also, `torch.no_grad()` disables autograd function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('./data/Tutorial_2_BasicNN.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    # declare that the model is about to evaluate\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_data:\n",
    "            images = images.view(-1, 28 * 28).to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Accuracy of Test Images: %f %%\" % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Images: 97.250000 %\n"
     ]
    }
   ],
   "source": [
    "test(model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate the Model\n",
    "It imports an image from test dataset and see if the learning is conducted properly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0, len(test_data)-1)\n",
    "X_single_data = Variable(test_data.test_data[r:r + 1].view(-1,28*28).float())\n",
    "Y_single_data = Variable(test_data.test_labels[r:r + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_prediction = model(X_single_data)\n",
    "plt.imshow(X_single_data.data.view(28, 28).numpy(), cmap='gray')\n",
    "\n",
    "print('Label: ', Y_single_data.data.view(1).numpy())\n",
    "print('Prediction: ', torch.max(single_prediction.data, 1)[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
