{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "#### Contents\n",
    "    1. Introduction\n",
    "    2. Import Data and Pre-processing\n",
    "    3. Make DataLoaders\n",
    "    4. Implementation of an RNN Model\n",
    "    5. Train and Validation\n",
    "    6. Test and Prediction\n",
    "    7. Reference\n",
    "\n",
    "## 1. Introduction\n",
    "With the advent of deep learning, problems that could not be solved with previous machine learning techniques have been resolved. **Sentiment Analysis** is one representative of those problems. While traditional techniques exploits hand-crafted programming, deep learning involves a huge amount of data with pre-processing done to do prediction and analysis. This task involves pre-processing of raw IMDB dataset and build a simple deep neural network model to analyze sentiment of a given data. \n",
    "\n",
    "## 2. Import Data and Pre-processing\n",
    "### 2.1 Import Data\n",
    "Download the from [here](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn/data). \n",
    "<br> Original Dataset can be downloaded at [here](https://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:1000])\n",
    "print()\n",
    "print(labels[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoding\n",
    "\n",
    "Before feeding the data into a deep learning model, they should be converted to numerical values because deep learning models cannot process language as human being. Conversion is called **encoding** and this involves each word converting to an integer. Before doing encoding, it is required to clean the data, as in deep learning tasks there is a famous maxim, **\"Garbage In, Garbage Out.\"**\n",
    "\n",
    "There are a few pre-processing steps, following:\n",
    "1. Remove punctuation.\n",
    "2. Split the text using \\n as the delimiter.\n",
    "3. Combine all the reviews back together into one big string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove punctuation\n",
    "reviews = reviews.lower()\n",
    "text = ''.join([c for c in reviews if c not in punctuation])\n",
    "print(punctuation)\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = text.split('\\n')\n",
    "text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build a Dictionary and Encode the Reviews\n",
    "\n",
    "Embedding lookup requires integer, thus make a **dictionary** that maps the words in the vocabulary to integers. Then through this dictionary, reviews can be converted into integers before feeding to the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# make a dictionary that maps vocabs to integers\n",
    "word_counts = Counter(words)\n",
    "vocab = sorted(word_counts, key = word_counts.get, reverse = True)\n",
    "\n",
    "vocab2idx = {vocab:idx for idx, vocab in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary:  74072\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of Vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews = []\n",
    "for review in reviews_split:\n",
    "    encoded_reviews.append([vocab2idx[vocab] for vocab in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of reviews:  25001\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of reviews: \", len(encoded_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Encode the labels\n",
    "Negative and Positive should be labelled to 0 and 1 (integers), respectively in order for them to be fed into the deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitted_labels = labels.split(\"\\n\")\n",
    "encoded_labels = np.array([\n",
    "    1 if label == \"positive\" else 0 for label in splitted_labels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of  labels:  25001\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of  labels: \", len(encoded_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Remove Outliers\n",
    "Reviews with length of 0 should be removed for processing. Then, padding will be applied to the remaining data so that all data have the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews:  1\n",
      "Maximum review length:  2514\n"
     ]
    }
   ],
   "source": [
    "length_reviews = Counter([len(x) for x in encoded_reviews])\n",
    "print(\"Zero-length reviews: \", length_reviews[0])\n",
    "print(\"Maximum review length: \", max(length_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of reviews:  25000\n",
      "The number of  labels:  25000\n"
     ]
    }
   ],
   "source": [
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [i for i, review in enumerate(encoded_reviews) if len(review) != 0]\n",
    "\n",
    "# Remove 0-length reviews and thier labels\n",
    "encoded_reviews = [encoded_reviews[i] for i in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[i] for i in non_zero_idx])\n",
    "\n",
    "print(\"The number of reviews: \", len(encoded_reviews))\n",
    "print(\"The number of  labels: \", len(encoded_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Padding Sequences\n",
    "\n",
    "To process both very long and very short reviews, it is necessary to pad short reviews with 0s to fit them to the specific length and truncate (or cut) long reviews to the `seq_length` words. `seq_length` is set to 200 in this notebook. In other words, all reviews should have the same length so padding and truncating are implemented.\n",
    "<br>For example, if there is review with length of 4 and `seq_length` is 10, it is converted to, \n",
    "- [64, 128, 256, 512]\n",
    "- [0, 0, 0, 0, 0, 0, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_padding(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 200\n",
    "padded_reviews = text_padding(encoded_reviews, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35  1819    16]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819    56    17]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5     1   730]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_reviews[:12, :12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make DataLoaders\n",
    "\n",
    "Split the data into train, validation and test sets with a ratio of 8:1:1. Then, `TensorDataset` and `DataLoader` functions will be used for processing the review and labels data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "train_length = int(len(padded_reviews) * ratio)\n",
    "\n",
    "X_train = padded_reviews[:train_length]\n",
    "y_train = encoded_labels[:train_length]\n",
    "\n",
    "remaining_x = padded_reviews[train_length:]\n",
    "remaining_y = encoded_labels[train_length:]\n",
    "\n",
    "test_length = int(len(remaining_x)*0.5)\n",
    "\n",
    "X_val = remaining_x[: test_length]\n",
    "y_val = remaining_y[: test_length]\n",
    "\n",
    "X_test = remaining_x[test_length :]\n",
    "y_test = remaining_y[test_length :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape of train review set:  (20000, 200)\n",
      "Feature shape of   val review set:  (2500, 200)\n",
      "Feature shape of  test review set:  (2500, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature shape of train review set: \", X_train.shape)\n",
    "print(\"Feature shape of   val review set: \", X_val.shape)\n",
    "print(\"Feature shape of  test review set: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 50\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train).to(device), torch.from_numpy(y_train).to(device))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(X_val).to(device), torch.from_numpy(y_val).to(device))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).to(device), torch.from_numpy(y_test).to(device))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)\n",
    "X_sample, y_sample = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   10,   206,   133,  ...,  1335,     8,     1],\n",
      "        [   59,   150,    69,  ...,    35,  1816, 44390],\n",
      "        [   10,   216,     3,  ...,     3, 16548,  1557],\n",
      "        ...,\n",
      "        [  136,     1,   846,  ...,     1,  1393,   281],\n",
      "        [    1,   421,     4,  ...,    29,     1,  8363],\n",
      "        [    0,     0,     0,  ...,   213,   280,     3]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print('Sample input size: ', X_sample.size())\n",
    "print('Sample input: \\n', X_sample)\n",
    "print()\n",
    "print('Sample label size: ', y_sample.size())\n",
    "print('Sample label: \\n', y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of an RNN Model\n",
    "\n",
    "![SentimentAnalysis](./images/SentimentAnalysis.png)\n",
    "\n",
    "Pre-processing including tokenizing is done so far. Now build a neural network model that predicts sentiment of reviews. \n",
    "\n",
    "First, an **embedding layer** converts word tokens into embeddings of a specific size. It performs as a lookup table instead of one-hot encoding, which often results in curse of dimensionality.\n",
    "<br> Second, an let be **LSTM** layer defined by `hidden_size` and `num_layers`.\n",
    "<br> Third, a desired output size is mapped from the output of LSTM layer via **a fully connected layer**.\n",
    "<br> Lastly, **a sigmoid activation layer** returns the outputs in the form of probability, 0 to 1.\n",
    "\n",
    "The above image is from [here](https://towardsdatascience.com/reading-between-the-layers-lstm-network-7956ad192e58).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # embedding and LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, \n",
    "                            hidden_size = hidden_dim, \n",
    "                            num_layers = num_layers, \n",
    "                            batch_first = True, \n",
    "                            dropout = 0.5, \n",
    "                            bidirectional = False)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, token, hidden):\n",
    "        \n",
    "        batch_size = token.size(0)\n",
    "        \n",
    "        # embedding and lstm output\n",
    "        out = self.embedding(token.long())\n",
    "        out, hidden = self.lstm(out, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        out = out.view(batch_size, -1)\n",
    "        \n",
    "        # get the last batch of labels\n",
    "        out = out[:, -1]\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)), \n",
    "                 Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "- `vocab_size` : size of vocabulary\n",
    "- `embedding_dim` : number of columns in the embedding lookup table\n",
    "- `hidden_dim` : number of units in the hidden layers of LSTM cells\n",
    "- `output_dim` : size of desired output, in this case 1 (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1 # +1 for the 0 padding + our word tokens\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Validation\n",
    "\n",
    "For loss function, `BCELoss` is used as **Binary Cross Entropy Loss** classifies by giving probability between 0 and 1.<br/>\n",
    "Adam optimizer is used with learning rate of 0.001. <br/>\n",
    "Besides, `torch.nn.utils.clip_grad_norm_(model.parameters(), clip = 5)` prevents the exploding and vanishing problem of gradient in RNN. `clip` is the maximum value of gradient to clip at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 | Step 100, Train Loss 0.6720, Val Loss 0.6383\n",
      "Epoch: 1/3 | Step 200, Train Loss 0.6623, Val Loss 0.6645\n",
      "Epoch: 1/3 | Step 300, Train Loss 0.6594, Val Loss 0.6032\n",
      "Epoch: 1/3 | Step 400, Train Loss 0.6522, Val Loss 0.6602\n",
      "Epoch: 2/3 | Step 100, Train Loss 0.6360, Val Loss 0.5446\n",
      "Epoch: 2/3 | Step 200, Train Loss 0.6201, Val Loss 0.6153\n",
      "Epoch: 2/3 | Step 300, Train Loss 0.6072, Val Loss 0.5183\n",
      "Epoch: 2/3 | Step 400, Train Loss 0.5962, Val Loss 0.5881\n",
      "Epoch: 3/3 | Step 100, Train Loss 0.5802, Val Loss 0.5452\n",
      "Epoch: 3/3 | Step 200, Train Loss 0.5650, Val Loss 0.5022\n",
      "Epoch: 3/3 | Step 300, Train Loss 0.5458, Val Loss 0.4594\n",
      "Epoch: 3/3 | Step 400, Train Loss 0.5299, Val Loss 0.4412\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    for i, (review, label) in enumerate(train_loader):\n",
    "        review, label = review.to(device), label.to(device)\n",
    "        \n",
    "        # Initialize Optimizer #\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make requires_grad = False of the Last Set of Hidden #\n",
    "        hidden = tuple([h.data for h in hidden])\n",
    "        \n",
    "        # Feed Forward #\n",
    "        output = model(review, hidden)\n",
    "        \n",
    "        # Calculate the Loss\n",
    "        loss = criterion(output.squeeze(), label.float())\n",
    "        \n",
    "        # Back Propagation #\n",
    "        loss.backward()\n",
    "        \n",
    "        # Prevent Exploding Gradient Problem #\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        # Update #\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Print Statistics #\n",
    "        if (i+1) % 100 == 0:\n",
    "            \n",
    "            ##################\n",
    "            ### Evaluation ###\n",
    "            ##################\n",
    "            \n",
    "            # initialize hidden state\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            for review, label in valid_loader:\n",
    "                review, label = review.to(device), label.to(device)\n",
    "                val_h = tuple([h.data for h in val_h])\n",
    "                output = model(review, val_h)\n",
    "                val_loss = criterion(output.squeeze(), label.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            print(\"Epoch: {}/{} | Step {}, Train Loss {:.4f}, Val Loss {:.4f}\".\n",
    "                  format(epoch+1, num_epochs, i+1, np.mean(train_losses), np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test\n",
    "\n",
    "It should be test on the test dataset for calculation of average loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader):\n",
    "    \n",
    "    # to calculate average of loss at the end\n",
    "    losses = []\n",
    "    corrects = 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    \n",
    "    # declare evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    for review, label in loader:\n",
    "        \n",
    "        review, label = review.to(device), label.to(device)\n",
    "        hidden = tuple([h.data for h in hidden])\n",
    "        output = net(review, hidden)\n",
    "        \n",
    "        # calculate losses\n",
    "        loss = criterion(output.squeeze(), label.float())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # convert output probabilities to classes, 0 or 1\n",
    "        pred = torch.round(output.squeeze())\n",
    "        \n",
    "        # compare with true labels\n",
    "        correct_tensor = pred.eq(label.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        corrects += np.sum(correct)\n",
    "        \n",
    "    accuracy = corrects / len(loader.dataset)\n",
    "        \n",
    "    print(\"Test Loss: {:.6f}\".format(np.mean(losses)))\n",
    "    print(\"Test Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.449302\n",
      "Test Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(net, sentence):\n",
    "    \n",
    "    ## First, given sentence should be tokenized at the first\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([c for c in sentence if c not in punctuation])\n",
    "    \n",
    "    words = sentence.split()\n",
    "    \n",
    "    tokens = []\n",
    "    tokens.append([vocab2idx[vocab] for vocab in words])\n",
    "    \n",
    "    # padding for constant length\n",
    "    sequence_length = 200\n",
    "    padded_sentence = text_padding(tokens, sequence_length)\n",
    "    \n",
    "    # convert to tensor in order to feed into the neural network\n",
    "    sentence_tensor = torch.from_numpy(padded_sentence).to(device)\n",
    "\n",
    "    # calculate the batch size\n",
    "    batch_size = sentence_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    # feed forward\n",
    "    output = net(sentence_tensor, h)\n",
    "    \n",
    "    # calculate rounded prediction\n",
    "    pred = torch.round(output.squeeze())\n",
    "    print(\"Predicted value is {:.4f}\".format(output.item()))\n",
    "    \n",
    "    if pred.item() == 0:\n",
    "        print(\"It is a negative review.\")\n",
    "    else:\n",
    "        print(\"It is a positive review.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This movie was horrible, don't wanna waste my time anymore so I went out of a theater!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value is 0.0573\n",
      "It is a negative review.\n"
     ]
    }
   ],
   "source": [
    "inference(model, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reference\n",
    "- [Udacity Deep Learning V2 PyTorch Repository](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/sentiment-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
